<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>MLLM Tutorial@ACM-MM2024</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/icon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Montserrat:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link href="assets/css/bulma.min.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Bootslander
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/bootslander-free-bootstrap-landing-page-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">
        <h1><a href="index.html"><span>MLLM Tutorial @ ACM MM 2024</span></a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!-- <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->
      </div>

      <nav id="navbar" class="navbar" style="background-color: #11af78;">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
<!--          <li><a class="nav-link scrollto" href="#features">Features</a></li>-->
<!--          <li><a class="nav-link scrollto" href="#gallery">Gallery</a></li>-->
          <li><a class="nav-link scrollto" href="#organizer">Organizer</a></li>
          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#video">Video</a></li>
          <li><a class="nav-link scrollto" href="#literature">Reading List</a></li>
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
          <li class="dropdown"><a href="#"><span>Tutorial Series</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="https://mllm2024.github.io/COLING2024/">COLING 2024 Tutorial</a></li>
              <li><a href="https://mllm2024.github.io/CVPR2024/">CVPR 2024 Tutorial</a></li>
<!--              <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-right"></i></a>-->
<!--                <ul>-->
<!--                  <li><a href="#">Deep Drop Down 1</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 2</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 3</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 4</a></li>-->
<!--                  <li><a href="#">Deep Drop Down 5</a></li>-->
<!--                </ul>-->
<!--              </li>-->
<!--              <li><a href="#">Drop Down 2</a></li>-->
<!--              <li><a href="#">Drop Down 3</a></li>-->
<!--              <li><a href="#">Drop Down 4</a></li>-->
            </ul>
          </li>
<!--          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>-->
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">

    <div class="container">
      <div class="">
        <div class="col-lg-7 pt-5 pt-lg-0 order-2 order-lg-1 d-flex align-items-center" style="min-width: 100%;">
          <div data-aos="zoom-out" class="aos-init aos-animate">
            <h1>From <span>Multimodal LLM</span> to Human-level AI: </h1>
            <h2>Architecture, Modality, Function, Instruction, Hallucination, Evaluation, Reasoning and Beyond</h2>
            <h3 style="color: #ffffff">28 October - 1 November 2024, Melbourne, Australia</h3>
          </div>
        </div>

      </div>
    </div>

    <svg class="hero-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28 " preserveAspectRatio="none">
      <defs>
        <path id="wave-path" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z">
      </path></defs>
      <g class="wave1">
        <use xlink:href="#wave-path" x="50" y="3" fill="rgba(255,255,255, .1)">
      </use></g>
      <g class="wave2">
        <use xlink:href="#wave-path" x="50" y="0" fill="rgba(255,255,255, .2)">
      </use></g>
      <g class="wave3">
        <use xlink:href="#wave-path" x="50" y="9" fill="#fff">
      </use></g>
    </svg>

  </section><!-- End Hero -->

  <main id="main">


    <!-- ======= Features Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>About</h2>
          <p>MLLM Tutorial</p>
        </div>

          <div style="width:95%">
            <p>Welcome to the <b>MLLM Tutorial</b> series on <b><a class="green-text" href="https://2024.acmmm.org/">ACM MM 2024</a></b>!</p>
            <p  style="text-align:justify">
              Artificial intelligence (AI) encompasses knowledge acquisition and real-world grounding across various modalities.
              As a multidisciplinary research field, multimodal large language models (MLLMs) have recently garnered growing interest in both academia and industry,
              showing an unprecedented trend to achieve human-level AI via MLLMs.
              These large models offer an effective vehicle for understanding, reasoning, and planning by integrating and modeling diverse information modalities,
              including language, visual, auditory, and sensory data.
              This tutorial aims to deliver a comprehensive review of cutting-edge research in MLLMs, focusing on following key areas:
              <b>MLLM architecture, modality, functionality</b>, <b>instructional learning</b>, <b>multimodal hallucination</b>, <b>MLLM evaluation</b> and <b>multimodal reasoning of MLLMs</b>.
              We will explore technical advancements, synthesize key challenges, and discuss potential avenues for future research.
            </p>
	<br><br>
<!--	  <p>Seattle local time zone (UTC-7): Tuesday, <u>June 18, 1:30 PM-6:00 PM</u></p>-->
<!--	   <p>Beijing time zone (UTC+8): Wednesday, <u>June 19, 4:30 AM - 9:00 AM</u></p>-->

        </div>
<br><br>

       <h2 class="title is-3">ðŸ””News</h2>
       <div class="content has-text-justified">
         <p>
           <b>ðŸ”¥[2024-10-02]: You can now visit the video record of the tutorial at <a href="https://www.youtube.com/watch?v=pHBT3zXxQX8">Youtube</a>!</b><br>
           <b>ðŸ”¥[2024-11-02]: We have released all the slides!</b><br>
           <b>ðŸ”¥[2024-10-22]: Also you may want to join our online Tutorial via this <S href="xxx">Zoom link</S>!</b><br>
           <b>ðŸ”¥[2024-10-20]: For in-person attendance, please come to <u>Meeting Room 210</u>, at Melbourne Convention and Exhibition Centre</b>.<br>
           <b>ðŸ”¥[2024-10-10]: This tutorial will be held on <u>Monday 28 October, 2024</u>.</b><br>
         </p>
         <br>
      </div>

	      
      </div>
    </section>
    <!-- End Features Section -->


    <!-- ======= Team Section ======= -->
    <section id="organizer" class="team">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Organizer</h2>
          <p>Presenters</p>
        </div>

        <div class="row" data-aos="fade-left" style="display: flex;flex-direction: row;align-items: center;justify-content: space-around;">

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="100">
              <div class="pic"><img src="assets/img/team/feihao.jpg" class="img-fluid" alt="https://haofei.vip/"></div>
              <div class="member-info">
                <h4><a href="https://haofei.vip">Hao Fei</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">National University of Singapore</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="200">
              <div class="pic"><img src="assets/img/team/lxt_cropped.png" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://lxtgh.github.io/">Xiangtai Li</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">ByteDance/Tiktok</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="assets/img/team/thl.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://hliu.cc/">Haotian Liu</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">xAI</span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="100">
              <div class="pic"><img src="assets/img/team/fxl_cropped.jpg" class="img-fluid" alt="https://haofei.vip/"></div>
              <div class="member-info">
                <h4><a href="https://fuxiaoliu.github.io">Fuxiao Liu</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">University of Maryland, College Park</span>
              </div>
            </div>
          </div>

        </div>
        <br>


        <div class="row" data-aos="fade-left" style="display: flex;flex-direction: row;align-items: center;justify-content: space-around;">

          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="assets/img/team/zzs.jpeg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://bcmi.sjtu.edu.cn/~zhangzs/">Zhuosheng Zhang</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">Shanghai Jiao Tong University</span>
              </div>
            </div>
          </div>


          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="assets/img/team/hanwang_profile.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">Nanyang Technological University</span>
              </div>
            </div>
          </div>


          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="https://kpzhang93.github.io/src/kpzhang.jpg" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://kpzhang93.github.io/">Kaipeng Zhang</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">Shanghai AI Lab</span>
              </div>
            </div>
          </div>


          <div class="col-lg-3 col-md-6 mt-5 mt-lg-0" style="width: 21%;">
            <div class="member" data-aos="zoom-in" data-aos-delay="300">
              <div class="pic"><img src="https://yanshuicheng.info/_next/static/media/bg-shuicheng-hr.b09f4fce.webp" class="img-fluid" alt=""></div>
              <div class="member-info">
                <h4><a href="https://yanshuicheng.info/">Shuicheng Yan</a></h4>
                <span style="font-size: 14px;color: #00cb6d;">Kunlun 2050 Research, Skywork AI</span>
              </div>
            </div>
          </div>


        </div>




      </div>
    </section>
    <!-- End Team Section -->


    <!-- ======= Details Section ======= -->
<!--    <section id="details" class="details">-->
<!--      <div class="container">-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4" data-aos="fade-right">-->
<!--            <img src="assets/img/details-1.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-4" data-aos="fade-up">-->
<!--            <h3>Voluptatem dignissimos provident quasi corporis voluptates sit assumenda.</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>-->
<!--              <li><i class="bi bi-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>-->
<!--              <li><i class="bi bi-check"></i> Iure at voluptas aspernatur dignissimos doloribus repudiandae.</li>-->
<!--              <li><i class="bi bi-check"></i> Est ipsa assumenda id facilis nesciunt placeat sed doloribus praesentium.</li>-->
<!--            </ul>-->
<!--            <p>-->
<!--              Voluptas nisi in quia excepturi nihil voluptas nam et ut. Expedita omnis eum consequatur non. Sed in asperiores aut repellendus. Error quisquam ab maiores. Quibusdam sit in officia-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4 order-1 order-md-2" data-aos="fade-left">-->
<!--            <img src="assets/img/details-2.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5 order-2 order-md-1" data-aos="fade-up">-->
<!--            <h3>Corporis temporibus maiores provident</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <p>-->
<!--              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate-->
<!--              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in-->
<!--              culpa qui officia deserunt mollit anim id est laborum-->
<!--            </p>-->
<!--            <p>-->
<!--              Inventore id enim dolor dicta qui et magni molestiae. Mollitia optio officia illum ut cupiditate eos autem. Soluta dolorum repellendus repellat amet autem rerum illum in. Quibusdam occaecati est nisi esse. Saepe aut dignissimos distinctio id enim.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4" data-aos="fade-right">-->
<!--            <img src="assets/img/details-3.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5" data-aos="fade-up">-->
<!--            <h3>Sunt consequatur ad ut est nulla consectetur reiciendis animi voluptas</h3>-->
<!--            <p>Cupiditate placeat cupiditate placeat est ipsam culpa. Delectus quia minima quod. Sunt saepe odit aut quia voluptatem hic voluptas dolor doloremque.</p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat.</li>-->
<!--              <li><i class="bi bi-check"></i> Duis aute irure dolor in reprehenderit in voluptate velit.</li>-->
<!--              <li><i class="bi bi-check"></i> Facilis ut et voluptatem aperiam. Autem soluta ad fugiat.</li>-->
<!--            </ul>-->
<!--            <p>-->
<!--              Qui consequatur temporibus. Enim et corporis sit sunt harum praesentium suscipit ut voluptatem. Et nihil magni debitis consequatur est.-->
<!--            </p>-->
<!--            <p>-->
<!--              Suscipit enim et. Ut optio esse quidem quam reiciendis esse odit excepturi. Vel dolores rerum soluta explicabo vel fugiat eum non.-->
<!--            </p>-->
<!--          </div>-->
<!--        </div>-->

<!--        <div class="row content">-->
<!--          <div class="col-md-4 order-1 order-md-2" data-aos="fade-left">-->
<!--            <img src="assets/img/details-4.png" class="img-fluid" alt="">-->
<!--          </div>-->
<!--          <div class="col-md-8 pt-5 order-2 order-md-1" data-aos="fade-up">-->
<!--            <h3>Quas et necessitatibus eaque impedit ipsum animi consequatur incidunt in</h3>-->
<!--            <p class="fst-italic">-->
<!--              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore-->
<!--              magna aliqua.-->
<!--            </p>-->
<!--            <p>-->
<!--              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate-->
<!--              velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in-->
<!--              culpa qui officia deserunt mollit anim id est laborum-->
<!--            </p>-->
<!--            <ul>-->
<!--              <li><i class="bi bi-check"></i> Et praesentium laboriosam architecto nam .</li>-->
<!--              <li><i class="bi bi-check"></i> Eius et voluptate. Enim earum tempore aliquid. Nobis et sunt consequatur. Aut repellat in numquam velit quo dignissimos et.</li>-->
<!--              <li><i class="bi bi-check"></i> Facilis ut et voluptatem aperiam. Autem soluta ad fugiat.</li>-->
<!--            </ul>-->
<!--          </div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>

&lt;!&ndash; End Details Section &ndash;&gt;-->


    <!-- ======= Gallery Section ======= -->
<!--    <section id="gallery" class="gallery">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Gallery</h2>-->
<!--          <p>Check our Gallery</p>-->
<!--        </div>-->

<!--        <div class="row g-0" data-aos="fade-left">-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="100">-->
<!--              <a href="assets/img/gallery/gallery-1.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-1.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="150">-->
<!--              <a href="assets/img/gallery/gallery-2.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-2.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="200">-->
<!--              <a href="assets/img/gallery/gallery-3.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-3.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="250">-->
<!--              <a href="assets/img/gallery/gallery-4.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-4.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="300">-->
<!--              <a href="assets/img/gallery/gallery-5.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-5.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="350">-->
<!--              <a href="assets/img/gallery/gallery-6.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-6.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="400">-->
<!--              <a href="assets/img/gallery/gallery-7.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-7.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-3 col-md-4">-->
<!--            <div class="gallery-item" data-aos="zoom-in" data-aos-delay="450">-->
<!--              <a href="assets/img/gallery/gallery-8.jpg" class="gallery-lightbox">-->
<!--                <img src="assets/img/gallery/gallery-8.jpg" alt="" class="img-fluid">-->
<!--              </a>-->
<!--            </div>-->
<!--          </div>-->

<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End Gallery Section -->

    <!-- ======= Testimonials Section ======= -->
<!--    <section id="testimonials" class="testimonials">-->
<!--      <div class="container">-->

<!--        <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">-->
<!--          <div class="swiper-wrapper">-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-1.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Saul Goodman</h3>-->
<!--                <h4>Ceo &amp; Founder</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Proin iaculis purus consequat sem cure digni ssim donec porttitora entum suscipit rhoncus. Accusantium quam, ultricies eget id, aliquam eget nibh et. Maecen aliquam, risus at semper.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Sara Wilsson</h3>-->
<!--                <h4>Designer</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Export tempor illum tamen malis malis eram quae irure esse labore quem cillum quid cillum eram malis quorum velit fore eram velit sunt aliqua noster fugiat irure amet legam anim culpa.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-3.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Jena Karlis</h3>-->
<!--                <h4>Store Owner</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Enim nisi quem export duis labore cillum quae magna enim sint quorum nulla quem veniam duis minim tempor labore quem eram duis noster aute amet eram fore quis sint minim.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-4.jpg" class="testimonial-img" alt="">-->
<!--                <h3>Matt Brandon</h3>-->
<!--                <h4>Freelancer</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Fugiat enim eram quae cillum dolore dolor amet nulla culpa multos export minim fugiat minim velit minim dolor enim duis veniam ipsum anim magna sunt elit fore quem dolore labore illum veniam.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--            <div class="swiper-slide">-->
<!--              <div class="testimonial-item">-->
<!--                <img src="assets/img/testimonials/testimonials-5.jpg" class="testimonial-img" alt="">-->
<!--                <h3>John Larson</h3>-->
<!--                <h4>Entrepreneur</h4>-->
<!--                <p>-->
<!--                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>-->
<!--                  Quis quorum aliqua sint quem legam fore sunt eram irure aliqua veniam tempor noster veniam enim culpa labore duis sunt culpa nulla illum cillum fugiat legam esse veniam culpa fore nisi cillum quid.-->
<!--                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>-->
<!--                </p>-->
<!--              </div>-->
<!--            </div>&lt;!&ndash; End testimonial item &ndash;&gt;-->

<!--          </div>-->
<!--          <div class="swiper-pagination"></div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End Testimonials Section -->




 <!-- ======= Schedule Section ======= -->
<section id="schedule" class="schedule">
  <div class="container">

    <div class="section-title" data-aos="fade-up">
      <h2>Schedule</h2>
      <p>PROGRAM</p>
    </div>

  <div class="container">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <p>
         The tutorial will be held on <b>Monday, 28 October, 2024</b> (all the times are based on <b>UTC/GMT +11 = Melbourne VIC local time</b>).
<!--           The tutorial has the following tentative schedule, which will be updated later. -->
        </p>
	<p><S>Also you can online join via <a href="#">Zoom Meeting</a></S></p>
      <br>
      <br>
      <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg" style="font-weight: bold;">
          <thead>
            <tr>
              <th class="tg-0pky" style="font-weight: bold;">Time</th>
              <th class="tg-0lax" style="font-weight: bold;">Section</th>
              <th class="tg-0lax" style="font-weight: bold;">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">09:00-09:05</td>
              <td class="tg-0lax">Part 1: Background and Introduction  <a href="assets/slides/MLLM-part1-Hao-intro.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Hao Fei</td>
            </tr>
            <tr>
              <td class="tg-0lax">09:05-09:35</td>
              <td class="tg-0lax">Part 2: MLLM Architecture&Modality <a href="assets/slides/MLLM-part2-Hao-architecture.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Hao Fei</td>
            </tr>
            <tr>
              <td class="tg-0lax">09:35-10:00</td>
              <td class="tg-0lax">Part 3: MLLM Functionality&Advances <a href="assets/slides/MLLM-part3-Xiangtai-advance.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Xiangtai Li</td>
            </tr>
	        <tr>
              <td class="tg-0lax">10:00-10:30</td>
              <td class="tg-0lax">Part 4: MLLM Instruction Tuning <a href="assets/slides/MLLM-part4-Haotian-instruction.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Haotian Liu</td>
            </tr>
            <tr>
              <td class="tg-0lax"></td>
              <td class="tg-0lax">Coffee Break, Q&A Session</a></td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">11:00-11:25</td>
              <td class="tg-0lax">Part 5: MLLM Hallucination  <a href="assets/slides/MLLM-part5-Fuxiao-hallucination.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Fuxiao Liu</td>
            </tr>
            <tr>
              <td class="tg-0lax">11:25-11:50</td>
              <td class="tg-0lax">Part 6: MLLM Evaluation&Generalist <a href="assets/slides/MLLM-part6-Hanwang-evaluation&generalist.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Hanwang Zhang</td>
            </tr>
            <tr>
              <td class="tg-0lax">11:50-12:10</td>
              <td class="tg-0lax">Part 7: MM Reasoning  <a href="assets/slides/MLLM-part7-Zhuosheng-reasoning.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Zhuosheng Zhang</td>
            </tr>
	    <tr>
              <td class="tg-0lax">12:10-12:30</td>
              <td class="tg-0lax">Part 8: Panel Discussion - From MM Generalist to Human-level AI</td>
              <td class="tg-0lax">All + Kaipeng Zhang + Shuicheng Yan</td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>

    </div>


    </div>

  </div>
</section>
<!-- End Schedule Section -->



	  
	  
<!-- ======= Video Section ======= -->
<section id="video" class="schedule">
<div class="container">

<div class="section-title" data-aos="fade-up">
  <h2>Tutorial Record</h2>
  <p>Video</p>
</div>

	
<div class="columns is-centered">
	<iframe width="1344" height="756" src="https://www.youtube.com/embed/pHBT3zXxQX8?si=L-sDJ9QKuwWGKbEG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

<!--<div class="columns is-centered">-->
<!--	<iframe width="1344" height="756" src="https://www.youtube.com/embed/pHBT3zXxQX8?si=L-sDJ9QKuwWGKbEG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>-->
<!--</div>-->

</div>
</section>
<!-- End Video Section -->




     <!-- ======= Literature Section ======= -->
    <section id="literature" class="literature">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Literature</h2>
          <p>Reading List</p>
        </div>

<!--          <p>Frequently Asked Questions</p>-->
        <div class="faq-list">

          <h4 class="title is-3">Architecture and Modality of LLMs and MLLMs</h4>
          <div style="margin-left: 3em">
            <ol>
              <li>
                OpenAI, 2023, <a href="https://openai.com/blog/chatgpt"><b>Introducing ChatGPT</b></a><br>
              </li>
              <li>
                OpenAI, 2023, <a href="https://arxiv.org/abs/2303.08774"><b>GPT-4 Technical Report</b></a><br>
              </li>
              <li>
                Alayrac, et al., 2022, <a href="https://arxiv.org/abs/2204.14198"><b>Flamingo: a Visual Language Model for Few-Shot Learning</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2301.12597"><b>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</b></a><br>
              </li>		    
              <li>
                Zhu, et al., 2023, <a href="https://arxiv.org/abs/2304.10592"><b>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2303.04671"><b>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</b></a><br>
              </li>		    
              <li>
                Shen, et al., 2023, <a href="https://arxiv.org/abs/2303.17580"><b>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2309.05519"><b>NExT-GPT: Any-to-Any Multimodal LLM</b></a><br>
              </li>
              <li>
                Fei, et al., 2024, <a href="http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf"><b>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</b></a><br>
              </li>
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2305.11846"><b>Any-to-Any Generation via Composable Diffusion</b></a><br>
              </li>
              <li>
                Girdhar, et al., 2023, <a href="https://arxiv.org/abs/2305.05665"><b>ImageBind: One Embedding Space To Bind Them All</b></a><br>
              </li>
              <li>
                Moon, et al., 2023, <a href="https://arxiv.org/abs/2309.16058"><b>AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model</b></a><br>
              </li>

              <li>
                Yao, et al., 2024, <a href="https://arxiv.org/abs/2408.01800"><b>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</b></a><br>
              </li>

              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2310.09478"><b>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2305.01278"><b>VPGTrans: Transfer Visual Prompt Generator across LLMs</b></a><br>
              </li>
              <li>
                Zhang, et al., 2024, <a href="https://arxiv.org/abs/2311.04498"><b>NExT-Chat: An LMM for Chat, Detection and Segmentation</b></a><br>
              </li>

              <li>
                Hu, et al., 2023, <a href="https://arxiv.org/abs/2308.12038"><b>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</b></a><br>
              </li>

              <li>
                Zhang, et al., 2024, <a href="https://arxiv.org/abs/2406.19389"><b>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</b></a><br>
              </li>

              <li>
                Bai, et al., 2023, <a href="https://arxiv.org/abs/2308.12966"><b>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</b></a><br>
              </li>

              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2311.03079"><b>CogVLM: Visual Expert for Pretrained Language Models</b></a><br>
              </li>
		    
              <li>
                Peng, et al., 2023, <a href="https://arxiv.org/abs/2306.14824"><b>Kosmos-2: Grounding Multimodal Large Language Models to the World</b></a><br>
              </li>

              <li>
                Dong, et al., 2023, <a href="https://arxiv.org/abs/2401.16420"><b>InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model</b></a><br>
              </li>
              
              <!-- modified start -->
              <li>
                Zhu, et al., 2023, <a href="https://arxiv.org/abs/2310.01852"><b>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</b></a><br>
              </li>
              <li>
                Ge, et al., 2023, <a href="https://arxiv.org/abs/2307.08041"><b>Planting a SEED of Vision in Large Language Model</b></a><br>
              </li>
              <li>
                Zhan, et al., 2024, <a href="https://arxiv.org/abs/2402.12226"><b>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</b></a><br>
              </li>
              <li>
                Kondratyuk, et al., 2023, <a href="https://arxiv.org/abs/2312.14125"><b>VideoPoet: A Large Language Model for Zero-Shot Video Generation</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2308.16692"><b>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</b></a><br>
              </li>
              <li>
                Zeghidour, et al., 2021, <a href="https://arxiv.org/abs/2107.03312"><b>SoundStream: An End-to-End Neural Audio Codec</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.03744"><b>Improved Baselines with Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Wu, et al., 2023, <a href="https://arxiv.org/abs/2303.04671"><b>Visual-ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2401.06395"><b>ModaVerse: Efficiently Transforming Modalities with LLMs</b></a><br>
              </li>
              <li>
                Lu, et al., 2023, <a href="https://arxiv.org/abs/2312.17172"><b>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</b></a><br>
              </li>
              <li>
                Bai, et al., 2023, <a href="https://arxiv.org/abs/2312.00785"><b>LVM: Sequential Modeling Enables Scalable Learning for Large Vision Models</b></a><br>
              </li>
              <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2302.14045"><b>Language Is Not All You Need: Aligning Perception with Language Models</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2305.06355"><b>VideoChat: Chat-Centric Video Understanding</b></a><br>
              </li>
              <li>
                Maaz, et al., 2023, <a href="https://arxiv.org/abs/2306.05424"><b>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2306.02858"><b>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</b></a><br>
              </li>
              <li>
                Lin, et al., 2023, <a href="https://arxiv.org/abs/2311.10122"><b>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</b></a><br>
              </li>
              <li>
                Qian, et al., 2024, <a href="https://arxiv.org/abs/2402.11435"><b>Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning</b></a><br>
              </li>
              <li>
                Hong, et al., 2023, <a href="https://arxiv.org/abs/2307.12981"><b>3D-LLM: Injecting the 3D World into Large Language Models</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2310.12945"><b>3D-GPT: Procedural 3D Modeling with Large Language Models</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2311.18651"><b>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</b></a><br>
              </li>
              <li>
                Xu, et al., 2023, <a href="https://arxiv.org/abs/2308.16911"><b>PointLLM: Empowering Large Language Models to Understand Point Clouds</b></a><br>
              </li>
              <li>
                Chen, et al., 2024, <a href="https://arxiv.org/abs/2401.12168"><b>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</b></a><br>
              </li>
              <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2304.12995"><b>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2305.11000"><b>SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2305.16107"><b>VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation</b></a><br>
              </li>
              <li>
                Rubenstein, et al., 2023, <a href="https://arxiv.org/abs/2306.12925"><b>AudioPaLM: A Large Language Model That Can Speak and Listen</b></a><br>
              </li>
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2310.13289"><b>SALMONN: Towards Generic Hearing Abilities for Large Language Models</b></a><br>
              </li>
              <li>
                Latif, et al., 2023, <a href="https://arxiv.org/abs/2310.13289"><b>Sparks of Large Audio Models: A Survey and Outlook</b></a><br>
              </li>
              <li>
                Luo, et al., 2022, <a href="https://arxiv.org/abs/2210.10341"><b>BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://doi.org/10.1101/2023.06.29.543848"><b>DrugGPT: A GPT-based Strategy for Designing Potential Ligands Targeting Specific Proteins</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2311.16079"><b>MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2304.06975"><b>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2310.14558"><b>AlpaCare:Instruction-tuned Large Language Models for Medical Application</b></a><br>
              </li>
              <li>
                Frey, et al., 2023, <a href="https://www.nature.com/articles/s42256-023-00740-3"><b>Neural Scaling of Deep Chemical Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2402.06852"><b>ChemLLM: A Chemical Large Language Model</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.12798"><b>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</b></a><br>
              </li>
              <li>
                Jiang, et al., 2023, <a href="https://arxiv.org/abs/2305.09645"><b>StructGPT: A General Framework for Large Language Model to Reason on Structured Data</b></a><br>
              </li>
              <li>
                Chen, et al., 2024, <a href="https://arxiv.org/abs/2402.08170"><b>LLaGA: Large Language and Graph Assistant</b></a><br>
              </li>
              <li>
                Koh, et al., 2023, <a href="https://arxiv.org/abs/2305.17216"><b>Generating Images with Multimodal Language Models</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2307.05222"><b>Generative Pretraining in Multimodality</b></a><br>
              </li>
              <li>
                Zheng, et al., 2023, <a href="https://arxiv.org/abs/2310.02239"><b>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</b></a><br>
              </li>
              <li>
                Dong, et al., 2023, <a href="https://arxiv.org/abs/2309.11499"><b>DreamLLM: Synergistic Multimodal Comprehension and Creation</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2311.05437"><b>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</b></a><br>
              </li>
              <li>
                Wang, et al., 2023, <a href="https://arxiv.org/abs/2311.16511"><b>GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation</b></a><br>
              </li>
              <li>
                Jin, et al., 2024, <a href="https://arxiv.org/abs/2402.03161"><b>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</b></a><br>
              </li>
              <li>
                Jin, et al., 2023, <a href="https://arxiv.org/abs/2311.08046"><b>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</b></a><br>
              </li>
              <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2311.17043"><b>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</b></a><br>
              </li>
              <li>
                Su, et al., 2023, <a href="https://arxiv.org/abs/2305.16355"><b>PandaGPT: One Model to Instruction-Follow Them All</b></a><br>
              </li>
              <li>
                Lyu, et al., 2023, <a href="https://arxiv.org/abs/2306.09093"><b>Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</b></a><br>
              </li>
              <li>
                Tang, et al., 2023, <a href="https://arxiv.org/abs/2311.18775"><b>CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2307.03601"><b>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</b></a><br>
              </li>
              <li>
                Yuan, et al., 2023, <a href="https://arxiv.org/abs/2312.10032"><b>Osprey: Pixel Understanding with Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Rasheed, et al., 2023, <a href="https://arxiv.org/abs/2311.03356"><b>GLaMM: Pixel Grounding Large Multimodal Model</b></a><br>
              </li>
              <li>
                Pi, et al., 2023, <a href="https://arxiv.org/abs/2305.14167"><b>DetGPT: Detect What You Need via Reasoning</b></a><br>
              </li>
              <li>
                Ren, et al., 2023, <a href="https://arxiv.org/abs/2312.02228"><b>PixelLM: Pixel Reasoning with Large Multimodal Model</b></a><br>
              </li>
              <li>
                Lai, et al., 2023, <a href="https://arxiv.org/abs/2308.00692"><b>Lisa: Reasoning segmentation via large language model</b></a><br>
              </li>
              <li>
                Chen, et al., 2023, <a href="https://arxiv.org/abs/2306.15195"><b>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</b></a><br>
              </li>
              <li>
                Munasinghe, et al., 2023, <a href="https://arxiv.org/abs/2311.13435"><b>PG-Video-LLaVA: Pixel Grounding in Large Multimodal Video Models</b></a><br>
              </li>
              <li>
                Yu, et al., 2023, <a href="https://arxiv.org/abs/2312.00589"><b>Merlin: Empowering Multimodal LLMs with Foresight Minds</b></a><br>
              </li>
              <li>
                Fu, et al., 2023, <a href="https://arxiv.org/abs/2306.13394"><b>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</b></a><br>
              </li>
              <li>
                Xu, et al., 2023, <a href="https://arxiv.org/abs/2306.09265"><b>LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</b></a><br>
              </li>
              <li>
                Ying, et al., 2024, <a href="https://arxiv.org/abs/2404.16006"><b>MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</b></a><br>
              </li>
              <li>
                Pan, et al., 2024, <a href="https://arxiv.org/abs/2405.01926"><b>Auto-Encoding Morph-Tokens for Multimodal LLM</b></a><br>
              </li>
              <li>
                Thagard, et al., 1997, <a href="https://link.springer.com/chapter/10.1007/978-94-017-0487-8_22"><b>Abductive reasoning: Logic, visual thinking, and coherence</b></a><br>
              </li>
              <li>
                Bavishi, et al., 2023, <a href="https://www.adept.ai/blog/fuyu-8b"><b>Fuyu-8B: A Multimodal Architecture for AI Agents</b></a><br>
              </li>

              <!-- modified end -->
            </ol>
        </div>

		    
        <br>
	  <h4 class="title is-3">Functionality and Recent Advances in MLLMs</h4>
	  <div style="margin-left: 3em">
	    <ol>
	      <li>
		OpenAI, 2023, <a href="https://openai.com/blog/chatgpt"><b>Introducing ChatGPT</b></a><br>
	      </li>
		  
            </ol>
        </div>
  
        <br>
          <h4 class="title is-3">Instruction Tuning & Hallucination</h4>
          <div style="margin-left: 3em">
            <ol>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2304.08485"><b>Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2306.14565"><b>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning</b></a><br>
              </li>
              <li>
                Gao, et al., 2023, <a href="https://arxiv.org/abs/2304.15010"><b>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</b></a><br>
              </li>
              <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2307.04087"><b>SVIT: Scaling up Visual Instruction Tuning</b></a><br>
              </li>
              <li>
                Ye, et al., 2023, <a href="https://arxiv.org/abs/2304.14178"><b>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</b></a><br>
              </li>
              <li>
                Yu, et al., 2023, <a href="https://arxiv.org/abs/2312.00849"><b>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</b></a><br>
              </li>
		<li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2311.10774"><b>MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</b></a><br>
              </li>
	       <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2304.10592"><b>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</b></a><br>
              </li>
	       <li>
                Liu, et al., 2023, <a href="https://arxiv.org/abs/2310.14566"><b>HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models</b></a><br>
              </li>
	       <li>
                Li, et al., 2023, <a href="https://arxiv.org/abs/2305.10355"><b>Evaluating Object Hallucination in Large Vision-Language Models</b></a><br>
              </li>
	      <li>
                Huang, et al., 2023, <a href="https://arxiv.org/abs/2306.13549"><b>Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey</b></a><br>
              </li>
	      <li>
                Yin, et al., 2023, <a href="https://arxiv.org/abs/2312.16602"><b>A Survey on Multimodal Large Language Models</b></a><br>
              </li>
	      <li>
                Yin, et al., 2023, <a href="https://arxiv.org/abs/2312.16602"><b>Woodpecker: Hallucination Correction for Multimodal Large Language Models</b></a><br>
              </li>
            </ol>
        </div>

    
        <br>
	  <h4 class="title is-3">MLLM Evaluation and Benchmarks</h4>
	  <div style="margin-left: 3em">
	    <ol>
	      <li>
		OpenAI, 2023, <a href="https://openai.com/blog/chatgpt"><b>Introducing ChatGPT</b></a><br>
	      </li>
		  
            </ol>
        </div>
  

        <br>
          <h4 class="title is-3">Multimodal Reasoning and Agent</h4>
          <div style="margin-left: 3em">
            <ol>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2302.00923"><b>Multimodal Chain-of-Thought Reasoning in Language Models</b></a><br>
              </li>
              <li>
                Zhao, et al., 2023, <a href="https://arxiv.org/abs/2309.07915"><b>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</b></a><br>
              </li>
              <li>
                Lu, et al., 2023, <a href="https://arxiv.org/abs/2304.09842"><b>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2309.11436"><b>You Only Look at Screens: Multimodal Chain-of-Action Agents</b></a><br>
              </li>
              <li>
                Sun, et al., 2023, <a href="https://arxiv.org/abs/2312.13286"><b>Generative multimodal models are in-context learners</b></a><br>
              </li>
              <li>
                Fei, et al., 2023, <a href="https://vitron-llm.github.io/"><b>VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</b></a><br>
              </li>
              <li>
                Wei, et al., 2023, <a href="https://arxiv.org/abs/2307.12626"><b>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</b></a><br>
              </li>
              <li>
                Zhang, et al., 2023, <a href="https://arxiv.org/abs/2311.11797"><b>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</b></a><br>
              </li>
               <li>
                Fei, et al., 2024, <a href="http://haofei.vip/VoT/"><b>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</b></a><br>
              </li>
              <li>
                Prystawski, et al., 2023, <a href="https://arxiv.org/abs/2304.03843"><b>Why think step by step? Reasoning emerges from the locality of experience</b></a><br>
              </li>
              <li>
                Gou, et al., 2023, <a href="https://arxiv.org/abs/2305.11738"><b>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</b></a><br>
              </li>
              <li>
                Tang, et al., 2024, <a href="https://arxiv.org/abs/2402.04247"><b>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</b></a><br>
              </li>
              <li>
                Yuan, et al., 2024, <a href="https://arxiv.org/abs/2401.10019"><b>R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</b></a><br>
              </li>
              <li>
                Hong, et al., 2023, <a href="https://arxiv.org/abs/2312.08914"><b>CogAgent: A Visual Language Model for GUI Agents</b></a><br>
              </li>
            </ol>
        </div>



      </div>
    </section><!-- End Literature Section -->



<!-- ======= Citation Section ======= -->
    <section id="citation" class="citation">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>Citation</h2>
          <p>Citation</p>
        </div>

          <div style="width:95%">
            <p  style="text-align:justify">
	    @inproceedings{fei2024multimodal,<br>
	  &nbsp title={From Multimodal LLM to Human-level AI: Modality, Instruction, Reasoning and Beyond},<br>
	  &nbsp author={Fei, Hao and Li, Xiangtai and Liu, Haotian and Liu, Fuxiao and Zhang, Zhuosheng and Zhang, Hanwang and Yan, Shuicheng},<br>
	  &nbsp booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},<br>
	  &nbsp pages={11289--11291},<br>
	  &nbsp year={2024}<br>
	}<br>
            </p>
        </div>


      </div>
    </section>
    <br>
    <!-- End Citation Section -->
	  
    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact section-bg">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
            <h2>Contact</h2>
	    <p>Contact us</p>
	</div>

        <div>
          <h5>Join and post at our <a href="https://groups.google.com/g/mllm24">Google Group</a>!</h5>
          <h5>Email the organziers at <a href="mailto:mllm24@googlegroups.com">mllm24@googlegroups.com</a> .</h5>

        </div>

     </div>

    </section><!-- End Contact Section -->




    <!-- ======= Cite Section ======= -->
<!--    <section id="cite" class="faq section-bg">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Cite</h2>-->
<!--          <p>BibTeX</p>-->
<!--        </div>-->

<!--      <div class="container is-max-desktop content">-->

<!--            <pre><code>@article{ retrieval-lm-tutorial,-->
<!--              author    = { Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi },-->
<!--              title     = { ACL 2023 Tutorial: Retrieval-based Language Models and Applications },-->
<!--              journal   = { ACL 2023 },-->
<!--              year      = { 2023 },-->
<!--            }</code></pre>-->
<!--        </div>-->


<!--        <div class="faq-list">-->
<!--          <ul>-->
<!--            <li data-aos="fade-up">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" class="collapse" data-bs-target="#faq-list-1">Non consectetur a erat nam at lectus urna duis? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-1" class="collapse show" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Feugiat pretium nibh ipsum consequat. Tempus iaculis urna id volutpat lacus laoreet non curabitur gravida. Venenatis lectus magna fringilla urna porttitor rhoncus dolor purus non.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="100">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-2" class="collapsed">Feugiat scelerisque varius morbi enim nunc? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-2" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Dolor sit amet consectetur adipiscing elit pellentesque habitant morbi. Id interdum velit laoreet id donec ultrices. Fringilla phasellus faucibus scelerisque eleifend donec pretium. Est pellentesque elit ullamcorper dignissim. Mauris ultrices eros in cursus turpis massa tincidunt dui.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="200">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-3" class="collapsed">Dolor sit amet consectetur adipiscing elit? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-3" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Eleifend mi in nulla posuere sollicitudin aliquam ultrices sagittis orci. Faucibus pulvinar elementum integer enim. Sem nulla pharetra diam sit amet nisl suscipit. Rutrum tellus pellentesque eu tincidunt. Lectus urna duis convallis convallis tellus. Urna molestie at elementum eu facilisis sed odio morbi quis-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="300">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-4" class="collapsed">Tempus quam pellentesque nec nam aliquam sem et tortor consequat? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-4" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Molestie a iaculis at erat pellentesque adipiscing commodo. Dignissim suspendisse in est ante in. Nunc vel risus commodo viverra maecenas accumsan. Sit amet nisl suscipit adipiscing bibendum est. Purus gravida quis blandit turpis cursus in.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--            <li data-aos="fade-up" data-aos-delay="400">-->
<!--              <i class="bx bx-help-circle icon-help"></i> <a data-bs-toggle="collapse" data-bs-target="#faq-list-5" class="collapsed">Tortor vitae purus faucibus ornare. Varius vel pharetra vel turpis nunc eget lorem dolor? <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>-->
<!--              <div id="faq-list-5" class="collapse" data-bs-parent=".faq-list">-->
<!--                <p>-->
<!--                  Laoreet sit amet cursus sit amet dictum sit amet justo. Mauris vitae ultricies leo integer malesuada nunc vel. Tincidunt eget nullam non nisi est sit amet. Turpis nunc eget lorem dolor sed. Ut venenatis tellus in metus vulputate eu scelerisque.-->
<!--                </p>-->
<!--              </div>-->
<!--            </li>-->

<!--          </ul>-->
<!--        </div>-->

<!--      </div>-->
<!--    </section>-->
    <!-- End F.A.Q Section -->





    <!-- ======= Contact Section ======= -->
<!--    <section id="contact" class="contact">-->
<!--      <div class="container">-->

<!--        <div class="section-title" data-aos="fade-up">-->
<!--          <h2>Contact</h2>-->
<!--          <p>Contact Us</p>-->
<!--        </div>-->

<!--        <div class="row">-->

<!--          <div class="col-lg-4" data-aos="fade-right" data-aos-delay="100">-->
<!--            <div class="info">-->
<!--              <div class="address">-->
<!--                <i class="bi bi-geo-alt"></i>-->
<!--                <h4>Location:</h4>-->
<!--                <p>A108 Adam Street, New York, NY 535022</p>-->
<!--              </div>-->

<!--              <div class="email">-->
<!--                <i class="bi bi-envelope"></i>-->
<!--                <h4>Email:</h4>-->
<!--                <p>info@example.com</p>-->
<!--              </div>-->

<!--              <div class="phone">-->
<!--                <i class="bi bi-phone"></i>-->
<!--                <h4>Call:</h4>-->
<!--                <p>+1 5589 55488 55s</p>-->
<!--              </div>-->

<!--            </div>-->

<!--          </div>-->

<!--          <div class="col-lg-8 mt-5 mt-lg-0" data-aos="fade-left" data-aos-delay="200">-->

<!--            <form action="forms/contact.php" method="post" role="form" class="php-email-form">-->
<!--              <div class="row">-->
<!--                <div class="col-md-6 form-group">-->
<!--                  <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>-->
<!--                </div>-->
<!--                <div class="col-md-6 form-group mt-3 mt-md-0">-->
<!--                  <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>-->
<!--                </div>-->
<!--              </div>-->
<!--              <div class="form-group mt-3">-->
<!--                <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" required>-->
<!--              </div>-->
<!--              <div class="form-group mt-3">-->
<!--                <textarea class="form-control" name="message" rows="5" placeholder="Message" required></textarea>-->
<!--              </div>-->
<!--              <div class="my-3">-->
<!--                <div class="loading">Loading</div>-->
<!--                <div class="error-message"></div>-->
<!--                <div class="sent-message">Your message has been sent. Thank you!</div>-->
<!--              </div>-->
<!--              <div class="text-center"><button type="submit">Send Message</button></div>-->
<!--            </form>-->

<!--          </div>-->

<!--        </div>-->

<!--      </div>-->
<!--    </section>&lt;!&ndash; End Contact Section &ndash;&gt;-->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
<!--    <div class="footer-top">-->
<!--      <div class="container">-->
<!--        <div class="row">-->

<!--          <div class="col-lg-4 col-md-6">-->
<!--            <div class="footer-info">-->
<!--              <h3>Bootslander</h3>-->
<!--              <p class="pb-3"><em>Qui repudiandae et eum dolores alias sed ea. Qui suscipit veniam excepturi quod.</em></p>-->
<!--              <p>-->
<!--                A108 Adam Street <br>-->
<!--                NY 535022, USA<br><br>-->
<!--                <strong>Phone:</strong> +1 5589 55488 55<br>-->
<!--                <strong>Email:</strong> info@example.com<br>-->
<!--              </p>-->
<!--              <div class="social-links mt-3">-->
<!--                <a href="#" class="twitter"><i class="bx bxl-twitter"></i></a>-->
<!--                <a href="#" class="facebook"><i class="bx bxl-facebook"></i></a>-->
<!--                <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>-->
<!--                <a href="#" class="google-plus"><i class="bx bxl-skype"></i></a>-->
<!--                <a href="#" class="linkedin"><i class="bx bxl-linkedin"></i></a>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->

<!--          <div class="col-lg-2 col-md-6 footer-links">-->
<!--            <h4>Useful Links</h4>-->
<!--            <ul>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Home</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">About us</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Services</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Terms of service</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Privacy policy</a></li>-->
<!--            </ul>-->
<!--          </div>-->

<!--          <div class="col-lg-2 col-md-6 footer-links">-->
<!--            <h4>Our Services</h4>-->
<!--            <ul>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Design</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Development</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Product Management</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Marketing</a></li>-->
<!--              <li><i class="bx bx-chevron-right"></i> <a href="#">Graphic Design</a></li>-->
<!--            </ul>-->
<!--          </div>-->

<!--          <div class="col-lg-4 col-md-6 footer-newsletter">-->
<!--            <h4>Our Newsletter</h4>-->
<!--            <p>Tamen quem nulla quae legam multos aute sint culpa legam noster magna</p>-->
<!--            <form action="" method="post">-->
<!--              <input type="email" name="email"><input type="submit" value="Subscribe">-->
<!--            </form>-->

<!--          </div>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Bootslander</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/bootslander-free-bootstrap-landing-page-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
